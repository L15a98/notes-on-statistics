# Information Criteria

## Disclaimer
The purpose of this post is to provide some intuition about the information criteria presented in the "Statistical Rethinking" book. I deliberately oversimplified and overgeneralized certain aspects to paint a "bigger picture".  If you are interested in further details, I compiled a list of papers that provide them.

## General idea
The general formula for all information criteria discussed below is
$$-2\cdot log(\frac{goodness~of~fit}{model~complexity})$$

The goodness-of-fit in the numerator is the likelihood, the joint probability of observing the model given each data point $\prod_i p(\Theta|y_i)$. The denominator expresses model complexity, i.e., its flexibility in fitting the sample and, therefore, its tendency to overfit. Thus, the fraction itself is goodness-of-fit per unit of model complexity. This is like miles-per-gallon for car efficiency, so better models are more efficient models, churning out more goodness per complexity.

The numerator is the same for all information criteria discussed below and they differ in how they compute the model complexity.

## Why smaller deviance / information criteria values means better
Because the goodness-of-fit is a product of probabilities, it itself ranges from 0 to 1. The denominator is a positive number above 1. This means that the fraction ranges from 0 to 1. The $log()$ bit flips and non-linearly scales it. The minus sign flips it back and we end up with smaller numbers meaning better fit. There is also $2$ in there but you can read about it [below](#two-in-deviance).
```{r echo=FALSE}
library(ggplot2)
the_log <- data.frame(p = seq(0.0001, 1, length.out = 1000))

ggplot(data=the_log, aes(x = p, y = -log(p))) + 
  geom_line()
```



## Akaike Information Criterion (AIC)
The formula most commonly used is
$$ AIC = -2\cdot log(p(\Theta|y)) + 2k $$
where $k$ is the number of parameters of the model. If you are not a mathematician who is used to translate back-and-forth using logarithms, you may fail spot to the ratio I was talking about earlier. For this, you need to keep in mind that $log(\frac{a}{b}) = log(a) - log(b)$ and that $a = log(exp(a))$. Let us re-arrange a bit to get the log-ratio back:
$$ AIC = -2\cdot log(p(\Theta|y)) + 2k $$
$$ AIC = -2 (log(p(\Theta|y)) - k)$$
$$ AIC = -2 (log(p(\Theta|y)) - log(exp(k))$$
$$ AIC = -2 \cdot log(\frac{p(\Theta|y)}{exp(k)})$$
And here it is, the log-ratio I've promised! As you can see, AIC assumes that model complexity grows exponentially with the number of parameters. In other words, adding another parameter increases complexity ~$2.718$ times.

If you are to use AIC, the current recommendation is to _correct it_ with an extra penalty for the size of the sample
$$AICc = AIC + \frac{2k^2 + 2k}{n - k - 1}$$
where $n$ is the sample size.

## Bayesian information criterion (BIC) 
A.k.a. Schwarz information criterion (SIC, SBC, SBIC). The motivation is the same as with AIC but the complexity term, in addition to the number of parameters, also reflects the sample size $n$.

$$BIC = -2\cdot log(p(\Theta|y)) + log(n) k $$

Let us do re-arranging again
$$BIC = -2\cdot log(p(\Theta|y)) + log(n) k $$
 $$BIC = -2 \left( log(p(\Theta|y)) + log(n) \frac{k}{2} \right) $$
$$BIC = -2 \left( log(p(\Theta|y)) + log \left(exp(log(n) \cdot \frac{k}{2} \right) \right) $$
 
For the complexity term, we need to keep in mind that $exp(a \cdot b) = exp(a)^b$. Thus, 
$$exp \left(log(n) \cdot \frac{k}{2} \right)= exp(log(n)) ^ \frac{k}{2} = n^\frac{k}{2}$$
Putting the complexity term back, we get
$$BIC = -2 \left( log(p(\Theta|y)) + log \left(n^\frac{k}{2} \right) \right)$$
$$BIC = -2 \cdot log \left(\frac{p(\Theta|y)}{n^\frac{k}{2}} \right)$$
Thus, we end up with very similar power law complexity term which uses sample size instead of Euler's number as the base.

## Problem of AIC and BIC: one size may not fit all

Both AIC and BIC assume that model complexity and flexibility, that leads to overfitting, is reflected in the number of parameters _k_. However, this is a fairly indirect measure of a model flexibility, based on how models in general tend to overfit data in general. But you probably want to know how your specific model uses its parameters to fit your specific sample and how much overfitting you should expect in that specific case. Because even if a parameter is present in the model, it may not be able to fully use in case of regularization or multilevel models.

Regularization, in form of strong priors, lasso/ridge regression, etc., restricts the range of values that a given parameter can take. Thus, a model cannot exploit it as much as _other_ parameters and will be less able to use it to improve fit to the sample. Similarly, in hierarchical multilevel modeling, you may have dozens or hundreds of parameters that describe intercepts and/or slopes for individual participants (random factors, in general), but most of them could be trivially average (same as or very similar to the group average) and contribute little to the actual fit. In these cases, a simple raw count, which treats all parameters as equals, will overestimate model complexity. 

The desire to go beyond one-size-fits-all approach and be as model- and data-specific led to development of deviance information criterion (DIC) and widely-applicable information criterion (WAIC). Both use posterior distribution of in-sample deviance / total log-likelihood and base their penalty on how variable this posterior distribution is. Higher variance, meaning that a model very different fits ranging from excellent to terrible, hints that model is too flexible for its own good and leads to higher complexity estimate (penalty for overfitting). Conversely, very similar posterior deviance (low variance) means that model is too restricted to fine-tune itself to the sample and its complexity is low.

If you understand why variance of the posterior distribution of divergence is related to models' flexibility and, therefore, to the number of effective parameters, just skip the next section. If not, I came up with a musical instruments metaphor that I and at least some people I've tested it upon found useful and intuitive.

## Musical instruments metaphor
Imagine that you are trying to play a song that you have just heard (when I was coming up with the metaphor it was Christmas, so I had "Jingle Bells" or "O Tannenbaum" in mind). But the only instrument you have is a triangle. This is not a particularly flexible instruments pitch-wise, so your rendition of that song will not be very good (your model underfits the data). The good news is that even if you do your worst and do not really try, no one will notice because your worst performance will sound very much like your best one. Simply because it is very hard to make songs sound different using a triangle. Thus, if you play that song many times, trying different versions of it with us judging how close you are to the original, your quality score will be very similar (low variance of the deviance for fitting to sample).

But what if I give you an instrument that can vary the pitch at least a bit, like a xylophone for children. Now you can try and your version of the music will sound much more like the original. But, it also gives you an opportunity to make a mess of it, so your rendition will sound nothing like the music you just heard. The instruments increases the difference between the best and the worst possible performance, so the variance of your performances (on how close they are to the original) also increases (higher variance of the deviance). A more flexible instrument will make the difference even bigger. Think violin or trombone which are not restricted to the scale, so you can play any sound in between and you can match the music you just heard exactly. Imagine that the music your just hear does have this odd off-the-scale sounds. Was a defect of the turntable, which cannot go at constant speed, so overall pitch wobbles overtime (noise), or is it an experimental music piece designed to sound odd (signal)? If you do not know for sure, you will try to play as close to the original (music your heard) as possible, matching those off-scale sounds. And, because you can play any sound, your range of possible performance is even larger from one-to-one to "please, have mercy and stop!" (even larger variance of deviance).

So, variance of your performance (posterior divergence) reflects how flexible your instrument is by why is it indicative of the _effective_ number of parameters? Here are regularizing priors in the world of music instrument metaphor. Imagine that in addition to the triangle, I also give you a rubber bell (it was used in a super-secret meeting in Stanislaw Lem's Eleventh Voyage of Ijon Tichy, so that no one would hear when they ring that bell!). Now, technically you now have two instruments (your number of parameters is 2) but that bell does not affect your performance (we put very strong regularizing priors so that coefficients are zero or something very-very close to zero). So your ability to play the song did not change and your variance of performance stays the same. Two actual instruments, but only one "effective" one. Or, I give you a piano but allow you to use only one octave and only white keys. Yes, you have a piano but with this regularization it is as complex as as kids' xylophone. The _potential_ number of notes you can play is great (AIC and BIC would be very impressed and slap a heavy penalty on you) but the actual "effective" range is small. Or, you regularize yourself to play scale only notes using violin (something you learn to do). In all of these cases, you deliberately restrict yourself. But why? Why not just play as you heard it (why not fit as well as you can?)? Because if you song is short (sample is small), regularization based on your knowledge about real life helps you to ignore the noise that is always present. E.g., that song is for kids' xylophone, so even if I heard notes outside of a single octave that was probably a problem with recording. Or, I never heard that piece for violin but I do know other works of this composer and they always use scale-only notes, so I should not use my violin to play off-scale sounds. (I have no idea what this food supplement will do to my height but I do not think that I'll grow by another 3 meters)

Multilevel models also limit the actual use of parameters. Imagine you heard a recording of a symphonic orchestra. Lots of violins but you figured that most of them actually play the same melody. So you can get away with using one violin score (sample group average) and assume that most violins play like that (most participants are very close to group average). Same goes if you hear a choir. Again, many people sing (lots of parameters!) but, mostly, in unison, so you do not need to create an individual score sheet (parameter) for each singer, just one per group of singers.

Wrapping up the metaphor, the more flexible your instrument is, the more variable your performance can be, the easier it is for you to mimic noise and imperfections of the recording that have nothing to do with the piece itself. But when you play it next time, matching the recording with all its noise perfectly, people who know the piece will be puzzled or may not even recognize it (poor out-of-sample predictions). Playing using a more limited instrument may make it easier for others to recognize the song! Thus, higher variance in performance accuracy (higher variance of deviance) indicates that you can overfit easily with that instrument (model) and you should be extra careful (impose higher penalty for complexity).

## Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)
The two are very similar, as both compute the model complexity based on posterior distribution of log likelihood. The key difference is that DIC sums the log likelihood for each model (sample) first and then computes the variance. WAIC computes variance of log likelihood per point and then sums those variances up. In the musical instrument metaphor, for DIC you perform the piece many times (generate many posterior samples), compute accuracy for each performance (deviance for a single sample), and then compute how variable they are. For WAIC, you go note by note. For each note you compute variance over all sample to see how consistent you are in playing it. Then, you sum this up. 



WAIC is more stable mathematically and is mode widely applicable (that's what statisticians tell us). Moreover, its advantage is that it explicitly recognizes that not all data points in your sample are equal. Some (outliers) are much harder to predict than others. And it is variance of log likelihood for these points that determines how much your model can overfit. An inflexible model will make a poor but consistent job (triangles don't care about pitch!), whereas a complex model can do anything from spot-on to terrible (violins can do anything). In short, you should use WAIC yourself but recognize DIC when you see it and think of it as somewhat less reliable WAIC, which is still better than AIC or BIC when you use regularizing priors and/or hierarchical models.

## Importance sampling

Importance sampling is mentioned in the chapter but is never explained, so here is a brief description. The core idea is to pretend that you sample from a distribution you need (but have no access to) by sampling from another distribution (the one you have access to) and "translating" the probabilities via _importance ratios_. What does this mean?

Imagine that you want to know an average total score for a given die after you throw it ten times. The procedure is as simple as it gets: you toss the die ten times, record the number you get on each throw, sum them up at the end. Repeat the same toss-ten-times-and-sum-it-up as many times as you want and compute your average. But what if you do not have access to that die because it is _the die_ and is kept under lock in International Bureau of Weights and Measures? Well, you have _a die_ which you can toss and you have a list of _importance ratios_ for each number. These _important ratios_ tell you how much more likely is the number of _the die_ (the one you are after) compared to _a die_ you have in your hand. Let's say the importance ratio for _1_ (so, number 1 comes up on top) is `3.0`. This means that whenever _your die_ gives you _1_, you assume that _the die_ came up _1_ on **three** throws. If the importance ratio for _2_ is 0.5, whenever you see _2_ on your die, you record only half the throw (_2_ comes up twice as rarely for real die than for your die, so two throws that give you _2_ amount to a single throw). This way you can toss your die and every toss equates to different number of throws that generated the same number for _the die_. So, you sample your die but record outcomes for the other die. Funny thing is that you don't even need to know how fair your die is and what is the probability of individual sides. As long as you know the importance ratios, keep tossing it and translating the probabilities, you will get the samples for the die you are interested in.

Note that if you toss your die ten times, the translated number of tosses for _the die_ does not need to add up to ten. Imagine that, just by chance, you got _1_ four times. Given the importance ratio of `3.0` that alone translates into twelve tosses. Solution? You _normalize_ your result by _sum of importance ratios_ and get back you ten tosses.

The very obvious catch is, _how do we know the importance ratios_? Well, that is situation specific. Sometimes, we can compute them because we know both distributions, it just that one is easier to sample than the target one (so, we optimize the use of computing power). Sometimes, as in case of PSIS/LOO below, we can use an approximation.

## Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)
The importance sampling I've described above is the key to the PSIS/LOO. The idea is the same, we want to sample from the posterior of the model that was fitted _without_ a specific data point $y_i$ (we write it as $p(\Theta_{-i,s}|y_i)$). But we do not really want to refit the model. Instead, we want to use what we already have, samples from the model that was fit on all the data, _including_ point $y_i$. So, we pull of the same importance sampling trick, sampling from  $p(\Theta_s|y_i)$ and translating it to $p(\Theta_{-i,s}|y_i)$. The only thing we need are importance factors^[Unfortunately, I did not yet follow the derivation of this proportionality, so I cannot explain why this is the case mathematically.] 
$$r_i = \frac{p(\Theta_{-i,s}|y_i)}{p(\Theta_s|y_i)} \propto \frac{1}{p(y_i|\Theta_s)}$$ 

In short, points that are hard for the model to predict (low $p(y_i|\Theta_s)$) are more important for estimating out-of-sample deviance. This is because if model does so poorly when it had a chance to account for an outlier point, it would do an even poorer job of predicting it, if it was fit without it. Higher importance ratio for these outlier points simple amplify that poor result, saying "things will be even worse". Conversely, 

Remember, that _importance ratio_ simply tells us how much more often do we expect to see this event. The higher importance ratio for poorly predicted points means that if unseen 
we expect such bad things to drag out deviance (goodness-of-fit) down even more than it currently does. 

Conversely, lower importance ratio for well-predicted points means that they are so typical that ignoring them won't 

To understand why, let us think about what will happen to the model fit with and without a problematic outlier point. When the point is in the sample, it drags a model away from the majority of "easy" data points. It is poorly predicted but at does get some attention. If it is _not_ in the sample, not only a model does not use available parameters to fit it (it does not know about it), it uses them to better fit other points, moving away from a (potential) good prediction for that outlier. So, resources that were initially spend to predict the outlier are now spent on some other points. In other words, if a model poorly predicts a data point _in sample_, it will do even worse _out of sample_.



a model can concentrate on these "easy" points and has more freedom to (over)fit them more precisely. As a consequence, if we try to predict our outlier model, the performance will be bad and definitely worse than for a model that was fitted on sample 

will be unprepared for the outlier when it comes in the next sample.

## Bayes Factor
Not an information criterion. However, it is a popular way to compare Bayesian models. Compared to information criteria, the logic is reversed. In case of the information criteria, we are asking which **model fits data** the best given the penalty we impose for its complexity. In case of Bayes Factor, we already have two models (could be different models with different number of parameters or just with different parameter values) and we are interested how well the **data matches models** we already have.

Let's start with the Bayes theorem:
$$Pr(M|D)=\frac {\Pr(D|M)\Pr(M)}{\Pr(D)}$$
where, _D_ is data and _M_ is the model (hypothesis). The tricky part is the marginal probability (prior) of data $Pr(D)$. We hardly ever know it for sure, making computing the "correct" value for $Pr(M|D)$ problematic. When using posterior sampling, we side-step the issue by ignoring it and normalizing the posterior by the sum of the posterior distribution. Alternatively, when comparing two models, you can compute their ratio:
$${\frac {\Pr(D|M_{1})}{\Pr(D|M_{2})}}={\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}} \cdot {\frac {\Pr(M_{1})}{\Pr(M_{2})}}$$
here $\frac{\Pr(D|M_{1})}{\Pr(D|M_{2})}$ are **posterior odds**, $\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}$ is **Bayes Factor**, and $\frac {\Pr(M_{2})}{\Pr(M_{1})}$ are **prior odds**. The common $Pr(D)$ nicely cancels out! 

If you assume that both hypotheses/models are equally likely (you have flat priors), the prior odds are 1:1 and your posterior odds are equal to Bayes Factor or, vice versa, Bayes Factor is equal to posterior odds. This means you can just pick their likelihoods from the posterior sampled distribution and compute the ratio.

I am not a big fan of Bayes Factor for conceptual reasons. Although it can compare any two models (as long as the sample is the same), it looks a lot like a Bayesian version of a p-value and, therefore, lends itself naturally to the null-hypothesis testing. And, as far as my reading of literature in my field is concerned, this is how people most frequently use it, as a cooler Bayesian way of null-hypothesis testing. You have no worries about multiple comparisons (it is Bayesian, so no need for error correction!) and it can prove null hypothesis (it is the ratio, so flip it and see how much stronger _H0_ is)! There is nothing wrong with this per se but the advantage of Bayesian statistics and information criteria is that you do not _need_ to think in terms of null hypothesis testing and nested models. Adopting Bayes Factor may prevent you from seeing this and will allow you to continue doing same analysis just in a differently colored wrapper. Again, there is nothing wrong with exploratory analysis using null hypothesis testing until you can formulate a better model. But it should not be the _only_ way you approach modeling.

## The two in deviance {#two-in-deviance}
The $2$ is where because of how deviance is formally defined
$$D = -2\cdot log(p(\Theta|y))$$
and, as far as I understand, this scaling by $2$ is necessary when you compare two models M1 and M2, which is M1 plus _k_ additional parameters via the difference in deviance. In that specific case, you can compute whether difference between models' deviance is significant using chi-squared distribution with _k_ degrees of freedom. This is how _nested_ models are frequently compared, for example, using `anova()` function in R.
