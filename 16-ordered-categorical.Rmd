# Ordered Categorical Data, i.e., Likert-scales


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

One of a very popular type of response in psychology and social sciences are Likert-scale response. For example, you may be asked to respond on how attractive you find a person in a photo from 1 (very unattractive) to 7 (very attractive). Or to respond how satisfied you are with a service from 1 (very unsatisfied) to 4 (very satisfied). Or rate your confidence on a 5-point scale, etc. Likert-scale responses are extremely common and are quite often analyzed via linear models (i.e., _t_-test, repeated measures ANOVA, linear-mixed models). The purpose of these notes is to document both technical and, more importantly, conceptual problems this approach entails. 

## Conceptualization of responses: continuous variable discritized via a set of cut-points
First, let us think what responses correspond to as it will become very important once we discuss conceptual problems with using linear models for Likert-scale data. 

When we ask a participant to respond "On scale from 1 to 7, how attractive do you find the face in the photo?", we assume that there is a _continuous_ internal variable (encoded via a neural ensemble) that represents attractiveness of a face (or our satisfaction with service, or our confidence, etc.). The strength of that representation varies in a continuous manner from its minimum (e.g., baseline firing rate, if we assume that strength is encoded by spiking rate) to maximum (maximum firing rate for that neural ensemble). When we impose a seven-point scale on a participants, we force them to discretize this continuous variable, creating a _many-to-one_ mapping. In other words, a participant decides that values (intensities) within a particular range all get mapped on _1_, a different but adjacent range of higher values corresponds to _2_, etc. You can think about it as values within that range being "rounded" (regressed?) towards the mean. Or, equivalently, you can think in terms of cut-points that defined range limits. This is how the discretization is depicted in the figure below. In addition, our participant has a set of cut-points that discretize this signal, as depicted in the figure below. If the signal is below the first cut point, our participant response is "1". When it is between first and second cut points, the response is "2" and so on. When it is to the right of the last sixth cut point, it is "7". This conceptualization means that responses are an ordered categorical variable, as any underlying intensity for a response "1" is necessarily smaller than _any_ intensity for response "2" and both are smaller than, again, _any_ intensity for response "3".

```{r echo=FALSE}
intensity <- c(0.5, 1.7, 6.4)
ggplot() +
  geom_hline(yintercept = 1:6) +
  geom_hline(yintercept = c(0, 7), color="red", size=1)+ 
  geom_bar(data = tibble(x=1:length(intensity), y=intensity), aes(x=x, y=y), stat="identity") +
  scale_y_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:6, labels = 1:6)) + 
  scale_x_continuous(name = "Intensity to Response", breaks = 1:length(intensity), labels = sprintf("%.1fâ†’%d", intensity, ceiling(intensity))) +
  coord_flip() +
  labs(subtitle="Many-to-one mapping of a continuous variable on categorical responses")
```

As per usual, we assume that our continuous variable is noisy and its values can be described as being drawn from a normal distribution centered at the "true" intensity level^[Here, I will assume that cut-points are fixed and it is parameters of the normal distribution that get adjusted. The actual implementation of ordered categorical has it other way around, so that intensity always comes from a normal distribution centered at 0 and with a standard deviation of one and it is cut-points that get adjusted. The two are mathematically equivalent but I find the former to be more intuitive.]. Here, the consistency of responses will depend on the width (standard deviation) of this distribution. The broader this distribution and / or closer it is to a cut-point, the more variable discrete responses will be.

```{r echo=FALSE}
x <- seq(0, 7, length.out = 500)
gaussians <- bind_rows(
  tibble(x = x, label = "1") %>%
    mutate(y = dnorm(x, mean = 2.5, sd = 0.125)),
    tibble(x = x, label = "2") %>%
    mutate(y = dnorm(x, mean = 2.5, sd = 1)),
    tibble(x = x, label = "3") %>%
    mutate(y = dnorm(x, mean = 2, sd = 0.125))
  ) %>%
  mutate(Response = ifelse(x == 0, 1, ceiling(x)),
         Response = ifelse(Response > 7, 7, Response),
         Response = factor(Response, levels = 1:7))

ggplot() +
  geom_ribbon(data=gaussians, aes(x=x, ymax=y, fill=Response), ymin=0) +
  geom_vline(xintercept = 1:6) +
  geom_vline(xintercept = c(0, 7), color="red", size=1)+ 
  scale_x_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:6, labels = 1:6)) + 
  facet_wrap(label~., ncol=1) + 
  theme(strip.background = element_blank(),  strip.text.x = element_blank()) +
  scale_y_continuous(name = "", breaks = NULL)

```

Our goal is to recover both the cut-points and the normal distribution using only observed responses.


## Conceptual problem with linear models: we change out mind about what responses correspond to.
If we use a linear model (a _t_-test, a repeated-measures ANOVA, linear-mixed models, etc.) to fit responses as an outcome (dependent) variable, we change the narrative. Recall that our original (and very intuitive) conceptualization was that they reflect a many-to-one mapping between an underlying continuous variable and a discrete (ordered categorical) response. Instead, linear models assume a _one-to-one_ mapping between the internal variable and observed responses. For example, a linear model postulates that for a 7-point Likert scale participant _could have_ responded with 6.5, 3.14, or 2.71828 but, for whatever reason (sheer luck?), we only observed a handful of integer values. 

Notice that this is _not_ how we think participants behave. I think everyone^[Never say always!] would object to the idea that the limited repertoire of responses is due to endogenous processing rather than exogenous limitations imposed by an experimental design. Yet, this is how a _linear model_ thinks about it and, if you are not careful, it is easy to miss this change in the narrative. It is, however, important as it means that estimates produced by a linear model are about that alternative one-to-one kind of responses, not the many-to-one that you had in mind! That alternative is not a bad story per se, it is just a _different_ story that should not be confused with the original one.

This change in the narrative of what responses correspond to is also a problem if you want to use a (fitted) linear model to simulate the data. It will happily spit out real valued responses like 6.5, 3.14, or 2.71828 (if you feel lucky enough to expect an actual integer response, you'd better use this luck on playing an actual lottery). You have two options. First, you bite the bullet and take them at their face value, sticking to one-to-one mapping between an internal variable and an observed response. That let's you keep the narrative but means that real and ideal observers play by different rules. Their responses are different and that means your conclusions based on an ideal observer behavior are of limited use. Alternatively, you can round real-valued responses off to a closest integer getting categorical responses. Unfortunately, that means changing the narrative yet again. In this case, you fitted the model assuming one-to-one mapping but you use its predictions assuming many-to-one. Not good. It is really hard to understand what is going on, if you keep changing your mind on what a response means. A linear model will also generate out-of-range responses, like -1 or 8. Here, you have little choice but to clip them into the valid range, forcing the many-to-one mapping on at least some responses. Again, change of a narrative means that model fitting and model interpretation rely on different conceptualizations of what response is.

This may sound too conceptual but I suspect that few people, who use linear models on Likert-scale data, realize that their model is not doing what they think it is doing and, erroneously!, interpret one-to-one linear-model estimates as many-to-one. The difference may or may not be crucial but the question is: Why employ a model that does something different to what you need to begin with? Remember, using an appropriate model and interpreting it correctly is _your_ job, not that of a mathematical model or nor is it a job of a software package. 

## A technical problem: Data that bunches up.
When you use a linear model, you must assume that residuals are normally distributed. This is something that you may not be sure of _before_ you fit a specific model, as it is residuals not the data that must be normally distributed. However, is some cases you may be fairly certain that this will not be the case, such as when a variable has only a limited range of values and the mean (model prediction) is close to one of these limits. Whenever you have observations that are close to that hard limit, they will "bunch up" against it because they cannot go lower or higher than that (see the figure below for illustration).

Note that the presence of the limit per se is not a deal breaker for using linear models. Most physical measures cannot be negative^[Try coming up with one and don't say "temperature"!] but as long your observations are far away from it, you are fine.

and are i.i.d.: Independent and Identically Distributed. This assumption becomes problematic if your variable has a hard upper and/or lower limit. The data points near the limit will "bunch up". Imagine some physical measure, e.g., an average height of objects that is close to zero, e.g., 1 cm. There is simply more opportunities to be larger than 1 cm compared to being  


Height of an adult female in USA is 164Â±6.4 cm. 

## Another technical problem: Can we even compute mean, if cut-points are not equdistant?

