# Ordered Categorical Data, i.e., Likert-scales


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```


One of a very popular type of response in psychology and social sciences are Likert-scale response. For example, you may be asked to respond on how attractive you find a person in a photo from 1 (very unattractive) to 7 (very attractive). Or to respond how satisfied are you with a service from 1 (very unsatisfied) to 4 (very satisfied). Or rate your confidence on a 5-point scale, etc. Likert-scale reponses are extremely common and are still typically analyzed via linear models (i.e., repeated measures ANOVA or linear-mixed models). Below, I document what kind of underlying neural representation does a Likert-scale response conceptualize and what kind of assumptions one needs to make when using them with linear models.

## Conceptualization: continuous variable discritized via a set of cut points
When we ask a participant to respond "On scale from 1 to 7, how attractive do you find the face in the photo?", we assume that there is some kind of neural ensemble that represents attractiveness of a face (or our satisfaction with service, or our confidence, etc.). The strength of that representation varies in a continuous manner from its minimum (e.g., baseline firing rate, if we assume that strength is encoded by spiking rate) to maximum (maximum firing rate for that neural ensemble). In addition, our participant has a set of cut points that discretize this signal, as depicted in the figure below. If the signal is below the first cut point, our participant response is "1". When it is between first and second cut points, the response is "2" and so on. When it is to the right of the last sixth cut point, it is "7". This conceptualization means that responses are an ordered categorical variable, as any underlying intensity for a response "1" is necessarily smaller than _any_ intensity for response "2" and both are smaller than, again, _any_ intensity for response "3".

```{r echo=FALSE}
intensity <- c(0.5, 1.7, 6.4)
ggplot() +
  geom_hline(yintercept = 1:6) +
  geom_hline(yintercept = c(0, 7), color="red", size=1)+ 
  geom_bar(data = tibble(x=1:length(intensity), y=intensity), aes(x=x, y=y), stat="identity") +
  scale_y_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:6, labels = 1:6)) + 
  scale_x_continuous(name = "Intensity to Response", breaks = 1:length(intensity), labels = sprintf("%.1fâ†’%d", intensity, ceiling(intensity))) +
  coord_flip()
```

We also assume that our representation is noisy and can be described as being drawn from a normal distribution centered at the "true" intensity level. Here, the consistency of responses will depend on the width (standard deviation) of this distribution. The broader this distribution and / or closer it is to a cut point, the more variable responses will be.

```{r echo=FALSE}
x <- seq(0, 7, length.out = 500)
gaussians <- bind_rows(
  tibble(x = x, label = "1") %>%
    mutate(y = dnorm(x, mean = 2.5, sd = 0.125)),
    tibble(x = x, label = "2") %>%
    mutate(y = dnorm(x, mean = 2.5, sd = 1)),
    tibble(x = x, label = "3") %>%
    mutate(y = dnorm(x, mean = 2, sd = 0.125))
  ) %>%
  mutate(Response = ifelse(x == 0, 1, ceiling(x)),
         Response = ifelse(Response > 7, 7, Response),
         Response = factor(Response, levels = 1:7))

ggplot() +
  geom_ribbon(data=gaussians, aes(x=x, ymax=y, fill=Response), ymin=0) +
  geom_vline(xintercept = 1:6) +
  geom_vline(xintercept = c(0, 7), color="red", size=1)+ 
  scale_x_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:6, labels = 1:6)) + 
  facet_wrap(label~., ncol=1) + 
  theme(strip.background = element_blank(),  strip.text.x = element_blank()) +
  scale_y_continuous(name = "", breaks = NULL)

```

Our goal is to recover the underlying Gaussian distribution using only observed responses. Given that assume that intensity sampling distribution is indeed a Gaussian, it feels natural fit observed _responses_ using linear models and infer estimates for the mean and the standard deviation as estimates for central moment of that underlying distribution.

```{r}
# Fitting responses
```

However, such direct approach is not as straightforward as it looks because it confuses distribution of intensity levels, which we trying to fit, with distribution of responses. This leads to various problems and issues described below.


## Problem #1: linear model pretends that responses are continuous but intermediate values are simply not observed
This and the following problem may sound too conceptual and less technical. However, to me they are even more important because they highlight discrepancies between your assumptions and that of a linear model. If you ignore this difference, you would take estimates from a linear model and interpret them as if they correspond to how you conceptualized the process. As with all statistical models, making sure that you interpret them correctly is your job. Not that of a mathematical model or nor is it a job of a software package. 

When we fit a normal distribution to responses directly, we change how we conceptualize our underlying signal. Originally, we though about responses as a cruder version of intensity, discretized by an application of multiple cut points. I.e., there is a many-to-one mapping between intensity and responses. But if we fit a Gaussian directly to responses, we now assume that the mapping is one-to-one and responses correspond to (are proportional to) the intensity itself. In other words, responses are intensity and they are also continuous but our participants only use intensity levels / responses that correspond to "integer" levels of intensity. Other responses (1.2, 3.7, etc.) are equally possible but are merely never observed. In more a formal way, the model assumes that frequency of a response "4" corresponds to probability density of that response and not to the cumulative likelihood (area under the curve) between second and third cut points.



You may feel that this is not quite how you think about this, but this is what you are implying by using a linear model. At the end of the day, although this one-to-one mapping and odd selectivity of intensity levels are peculiar, one can postulate such a conceptualization. The problem is, it is a _different_ conceptualization from what we started with, so a linear model will correspond to that different conceptualization, not the one we actually want to recover.

## Floor and ceiling effects mean that mean is no longer an unbiased estimator
Our assumption is our signal has a certain range, that it is limited by minimal and maximal intensity levels that limited response as well. However, a linear model makes no such assumption, as it has support of infinity. This means that responses below 1 and above 7 (let's say that this is our maximal response level) are clipped. E.g., if our Gaussian is centered exactly on 7, all the responses above that will be 

## Heteroscedacity

## Can we even compute mean, if cut points are not equdistant?

## Model predicts nonsense and it is not clear how to turn it into sense