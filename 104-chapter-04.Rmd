# Solutions for Chapter 4

#### 4E1{-}
In the model definition below, which line is the likelihood?
$$\rightarrow ~ y_i \sim Normal(\mu, \sigma) ~ \leftarrow\\
\mu \sim Normal(0, 10) \\
\sigma \sim Exponential(1)$$

#### 4E2{-} 
In the model definition just above, how many parameters are in the posterior distribution?

**Two: $\mu$ and $\sigma$.

#### 4E3{-}
Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.

$$Pr(\mu, \sigma | y) = \frac{\prod_i Normal(y_i| \mu, \sigma) \cdot Normal(mu | 0, 10) \cdot Exponential(\sigma | 1)}
{\int \int \prod_i Normal(y_i| \mu, \sigma) \cdot Normal(mu | 0, 10) \cdot Exponential(\sigma | 1) \partial \mu \partial \sigma}$$


#### 4E4 {-}
In the model definition below, which line is the linear model?
$$y_i ∼ Normal(\mu, \sigma) \\
\rightarrow ~\mu_i = \alpha + \beta x_i ~\leftarrow\\
α ∼ Normal(0, 10) \\ 
β ∼ Normal(0, 1) \\
σ ∼ Exponential(2)$$

#### 4E5 {-} 
In the model definition just above, how many parameters are in the posterior distribution?

Three, $α$, $β$ (used to compute $\mu$), and $\sigma$.

#### 4M1{-}
For the model definition below, simulate observed y values from the prior (not the posterior).
$$y_i \sim Normal(\mu,\sigma) \\
\mu \sim Normal(0, 10) \\
\sigma \sim Exponential(1)$$
```{r 4M1}
library(ggplot2)

samples_N <- 1e4
mu <- rnorm(samples_N, 0, 10)
sigma <- rexp(samples_N, 1)
y_sim <- rnorm(samples_N, mu, sigma)

ggplot(data=NULL, aes(x=y_sim)) + 
  geom_histogram(bins=50) + 
  xlab("Simulated Y")
```

#### 4M2 {-}
Translate the model just above into a quap formula.

```{r 4M2}
quap_formula <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0,10),
  sigma ~ dexp(1))
```

#### 4M3 {-}
Translate the quap model formula below into a mathematical model definition.
```
y ~ dnorm( mu , sigma ),
mu <- a + b*x,
a ~ dnorm( 0 , 10 ),
b ~ dunif( 0 , 1 ),
sigma ~ dexp( 1 )
```
$$y_i ∼ Normal(\mu, \sigma) \\
\mu_i = \alpha + \beta x_i \\
α ∼ Normal(0, 10) \\ 
β ∼ Uniform(0, 1) \\
σ ∼ Exponential(1)$$

#### 4M4{-} 
A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

I assume _students_ as in _college students_. Therefore, you would expect an adult height, may be slightly lower, hence `170` for the intercept. You don't expect them to grow much, hence conservative slope for the effect of the year. I assume a fairly broad but strictly positive prior for $\sigma$.

$$h_i ∼ Normal(\mu, \sigma) \\
\mu_i = \alpha + \beta year_i \\
α ∼ Normal(170, 10) \\ 
β ∼ Normal(0, 1) \\
σ ∼ Exponential(0.1)$$

#### 4M5{-}
Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?

I would ensure that $\beta$ can be only positive, so $β ∼ Exponential(1)$.

#### 4M6{-}
Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?


Based on the density plot, I would pick a more conservative prior of $\sigma \sim Exponential(0.5)$ that drops off more rapidly and just barely includes $\sqrt(64)$.
```{r}
x <- seq(0, 10, length.out=100)
ggplot(data=NULL, aes(x=x, y= dexp(x, 0.5))) + 
  geom_line() +
  scale_x_continuous(breaks = 0:10) +
  ylab("Probability density")
```

#### 4M7{-} 
Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.

```{r 4M7-1}
# load data again, since it's a long way back
library(rethinking)
data(Howell1); 
d <- Howell1; 
d2 <- d[d$age >= 18,]

# define the average weight, x-bar
xbar <- mean(d2$weight)
# fit model

m4_3c <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20) ,
    b ~ dlnorm(0, 1) ,
    sigma ~ dunif(0, 50)) , 
  data=d2)

m4_3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(178, 20) ,
    b ~ dlnorm(0, 1) ,
    sigma ~ dunif(0, 50)) , 
  data=d2)

precis(m4_3)
```

```{r 4M7-2}
precis(m4_3c)
```
```{r 4M7-3}
round(vcov(m4_3), 3)
```
```{r 4M7-4}
round(vcov(m4_3c), 3)
```
```{r 4M7-5}
post <- extract.samples(m4_3)
a_map <- mean(post$a)
b_map <- mean(post$b)

w <- d2$weight
prediction <- a_map + b_map*w

ggplot(data = d2, aes(x=weight, y=height)) + 
  geom_point() +
  geom_line(data = NULL, aes(x=w, y=prediction))
```
```{r}
post <- extract.samples(m4_3c)
a_map <- mean(post$a)
b_map <- mean(post$b)

w <- d2$weight
prediction <- a_map + b_map*(w - xbar)

ggplot(data = d2, aes(x=weight, y=height)) + 
  geom_point() +
  geom_line(data = NULL, aes(x=w, y=prediction))
```
#### 4M8{-} 
In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights—change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?
