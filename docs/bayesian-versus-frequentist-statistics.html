<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Bayesian versus Frequentist Statistics | Notes on and Solutions for Statistical Rethinking</title>
  <meta name="description" content="Notes and exercise solutions for Statistical Rethinking book." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Bayesian versus Frequentist Statistics | Notes on and Solutions for Statistical Rethinking" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and exercise solutions for Statistical Rethinking book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Bayesian versus Frequentist Statistics | Notes on and Solutions for Statistical Rethinking" />
  
  <meta name="twitter:description" content="Notes and exercise solutions for Statistical Rethinking book." />
  

<meta name="author" content="Alexander Pastukhov" />


<meta name="date" content="2021-04-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="information-criteria.html"/>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes on Statistical Rethinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Precis</a></li>
<li class="chapter" data-level="2" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>2</b> Loss functions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="loss-functions.html"><a href="loss-functions.html#loss-function-the-concept"><i class="fa fa-check"></i><b>2.1</b> Loss function, the concept</a></li>
<li class="chapter" data-level="2.2" data-path="loss-functions.html"><a href="loss-functions.html#l0-mode"><i class="fa fa-check"></i><b>2.2</b> L0 (mode)</a></li>
<li class="chapter" data-level="2.3" data-path="loss-functions.html"><a href="loss-functions.html#l1-median"><i class="fa fa-check"></i><b>2.3</b> L1 (median)</a></li>
<li class="chapter" data-level="2.4" data-path="loss-functions.html"><a href="loss-functions.html#l2-mean"><i class="fa fa-check"></i><b>2.4</b> L2 (mean)</a></li>
<li class="chapter" data-level="2.5" data-path="loss-functions.html"><a href="loss-functions.html#l1-median-vs.-l2-mean"><i class="fa fa-check"></i><b>2.5</b> L1 (median) vs.Â L2 (mean)</a></li>
<li class="chapter" data-level="2.6" data-path="loss-functions.html"><a href="loss-functions.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.6</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.7" data-path="loss-functions.html"><a href="loss-functions.html#gaussian-in-frenquentist-versus-bayesian-statistics"><i class="fa fa-check"></i><b>2.7</b> Gaussian in frenquentist versus Bayesian statistics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html"><i class="fa fa-check"></i><b>3</b> Directed Acyclic Graphs and Causal Reasoning</a>
<ul>
<li class="chapter" data-level="3.1" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html#peering-into-a-black-box"><i class="fa fa-check"></i><b>3.1</b> Peering into a black box</a></li>
<li class="chapter" data-level="3.2" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html#turning-unconditional-dependence-into-conditional-independence"><i class="fa fa-check"></i><b>3.2</b> Turning unconditional dependence into conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collider-bias.html"><a href="collider-bias.html"><i class="fa fa-check"></i><b>4</b> Collider bias</a>
<ul>
<li class="chapter" data-level="4.1" data-path="collider-bias.html"><a href="collider-bias.html#multicollinearity"><i class="fa fa-check"></i><b>4.1</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.2" data-path="collider-bias.html"><a href="collider-bias.html#back-to-spurious-association"><i class="fa fa-check"></i><b>4.2</b> Back to spurious association</a></li>
<li class="chapter" data-level="4.3" data-path="collider-bias.html"><a href="collider-bias.html#chain-dag"><i class="fa fa-check"></i><b>4.3</b> Chain DAG</a></li>
<li class="chapter" data-level="4.4" data-path="collider-bias.html"><a href="collider-bias.html#take-home-message"><i class="fa fa-check"></i><b>4.4</b> Take-home message</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-haunted-dag.html"><a href="the-haunted-dag.html"><i class="fa fa-check"></i><b>5</b> The haunted DAG</a></li>
<li class="chapter" data-level="6" data-path="information-criteria.html"><a href="information-criteria.html"><i class="fa fa-check"></i><b>6</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="6.1" data-path="information-criteria.html"><a href="information-criteria.html#deviance"><i class="fa fa-check"></i><b>6.1</b> Deviance</a></li>
<li class="chapter" data-level="6.2" data-path="information-criteria.html"><a href="information-criteria.html#general-idea-information-criteria-as-miles-per-gallon"><i class="fa fa-check"></i><b>6.2</b> General idea: information criteria as miles-per-gallon</a></li>
<li class="chapter" data-level="6.3" data-path="information-criteria.html"><a href="information-criteria.html#akaike-information-criterion-aic"><i class="fa fa-check"></i><b>6.3</b> Akaike Information Criterion (AIC)</a></li>
<li class="chapter" data-level="6.4" data-path="information-criteria.html"><a href="information-criteria.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>6.4</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="6.5" data-path="information-criteria.html"><a href="information-criteria.html#problem-of-aic-and-bic-one-size-may-not-fit-all"><i class="fa fa-check"></i><b>6.5</b> Problem of AIC and BIC: one size may not fit all</a></li>
<li class="chapter" data-level="6.6" data-path="information-criteria.html"><a href="information-criteria.html#musical-instruments-metaphor"><i class="fa fa-check"></i><b>6.6</b> Musical instruments metaphor</a></li>
<li class="chapter" data-level="6.7" data-path="information-criteria.html"><a href="information-criteria.html#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><i class="fa fa-check"></i><b>6.7</b> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)</a></li>
<li class="chapter" data-level="6.8" data-path="information-criteria.html"><a href="information-criteria.html#importance-sampling"><i class="fa fa-check"></i><b>6.8</b> Importance sampling</a></li>
<li class="chapter" data-level="6.9" data-path="information-criteria.html"><a href="information-criteria.html#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><i class="fa fa-check"></i><b>6.9</b> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)</a></li>
<li class="chapter" data-level="6.10" data-path="information-criteria.html"><a href="information-criteria.html#bayes-factor"><i class="fa fa-check"></i><b>6.10</b> Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html"><i class="fa fa-check"></i><b>7</b> Bayesian versus Frequentist Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#choice-of-likelihood-both"><i class="fa fa-check"></i><b>7.1</b> Choice of likelihood (both)</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#linear-model-both"><i class="fa fa-check"></i><b>7.2</b> Linear model (both)</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#priors-optional-for-bayesian"><i class="fa fa-check"></i><b>7.3</b> Priors (optional for Bayesian)</a></li>
<li class="chapter" data-level="7.4" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#maximum-likelihood-estimate-both"><i class="fa fa-check"></i><b>7.4</b> Maximum-likelihood estimate (both)</a></li>
<li class="chapter" data-level="7.5" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#uncertainty-about-estimates-different-but-comparable"><i class="fa fa-check"></i><b>7.5</b> Uncertainty about estimates (different but comparable)</a></li>
<li class="chapter" data-level="7.6" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#model-comparison-via-information-criteria-both"><i class="fa fa-check"></i><b>7.6</b> Model comparison via information criteria (both)</a></li>
<li class="chapter" data-level="7.7" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#generating-predictions-both"><i class="fa fa-check"></i><b>7.7</b> Generating predictions (both)</a></li>
<li class="chapter" data-level="7.8" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#conclusions"><i class="fa fa-check"></i><b>7.8</b> Conclusions</a></li>
<li class="chapter" data-level="7.9" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#take-home-message-1"><i class="fa fa-check"></i><b>7.9</b> Take home message</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://alexander-pastukhov.github.io/">Alexander (Sasha) Pastukhov</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on and Solutions for Statistical Rethinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-versus-frequentist-statistics" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Bayesian versus Frequentist Statistics</h1>
<p>I suspect that many student who read âStatistical Rethinkingâ have a feeling that it is something completely different from what they have been learning in âtraditionalâ statistics classes. That Bayesian approach is more âhands-onâ and complicated, whereas ânormalâ statistics is simpler and easy to work with even it is âless powerful.â<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> Thus, the purpose of this note is to walk you through a typical statistical analysis and focus on practical differences and, more importantly, similarity of the two approaches.</p>
<div id="choice-of-likelihood-both" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Choice of likelihood (both)</h2>
<p>The very first we do is to look at the data and decide which distribution we will use to model the data / residuals be it normal, binomial, Poisson, beta, etc. That is the very first line of our models that goes like this
<span class="math display">\[
\color{red}{y_i \sim Normal(\mu_i, \sigma)} \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots \\
\sigma \sim Exponential(1)
\]</span></p>
<p>This decision is neither Bayesian, nor frequentist. This is a decision about the model that best describes the data, so it is independent of the inference method you will use. This is a decision that you are making even if you are using âprepackagedâ statistical tests like the t-test or ANOVA that assume normally distributed residuals<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>.</p>
</div>
<div id="linear-model-both" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Linear model (both)</h2>
<p>Next, you decide on the deterministic part of the model that expresses how a parameter of the distribution you chose on the previous step is computed from various predictors. E.g., for the linear regression with normally distributed residuals, you decide which predictors do you use to compute the mean. The model line would look something like this
<span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\color{red}{\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso} \\
\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots
\]</span>
Again, this is neither Bayesian, nor frequentist decision, it is a linear model decision. Chapters 4-6 and 8 concentrate on how to make this decision using directed-acyclic graphs (DAGs) and introduce concepts of multicollinearity, colliders and bias they can produce, backdoor paths and how to identify them, etc. They explain how you can make educated decision on which predictors to use based on your knowledge of the field or of the problem. At this stage you also decide on whether to normalize data, as it could make interpreting the model easier.</p>
<p>You always have to make this decision. For example, if you use the (repeated measures) ANOVA, you do need to decide which factors to use, whether to include interactions, should you transform the data to make coefficients directly interpretable, do you use a link function, etc.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
</div>
<div id="priors-optional-for-bayesian" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Priors (optional for Bayesian)</h2>
<p>Priors are a Bayesian way to regularize the model, so this is something you do need to think about when doing Bayesian statistics<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>. In a model this part would look something like this
<span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\color{red}{\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots}
\]</span></p>
<p>This is probably a decision that students worry about the most as it feels more subjective and arbitrary than other decisions, such as choice of the likelihood or predictors. Chapter 4 through 7 gave you multiple examples that there is nothing particularly arbitrary about these choices and that you can come up with a set of justifiable priors based on what you know about the topic or based on how your pre-processed the data<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>.</p>
<p>Still, I think for a lot of people ânormalâ statistics with its flat priors feels simpler and also more objective and, therefore, more trustworthy (âwe did not favor any specific range of values!â). If that is the case then use flat priors (but see the side note below) making Bayesian and frequentists models identical! For me, though, writing it down explicitly makes one realize that range <span class="math inline">\(-\infty, +\infty\)</span> is remarkably large to the point of being an obvious overkill
<span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\color{red}{\alpha \sim Uniform(-\infty, +\infty) \\
\beta_{x1} \sim Uniform(-\infty, +\infty) \\
\cdots}
\]</span></p>
<p>In short, Bayesian inference gives you an <em>option</em> to specify priors. You do not need to take on the option and can use flat frequentistâs priors.</p>
<p><em>Side note.</em> In reality, flat priors are never good priors. If there is sufficient data then, in most cases, the priors (flat or not) do not have much influence. However, if the data is limited then flat priors almost inevitably lead to overfitting as there is no additional information to counteract the effect of noise. This overfitting may feel more âobjectiveâ and âdata-drivenâ than a more conservative underfitting of data via strongly regularizing priors but the latter is more likely to lead to better out-of-sample predictions and, therefore, are more likely to be replicated.</p>
</div>
<div id="maximum-likelihood-estimate-both" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Maximum-likelihood estimate (both)</h2>
<p>Once you fitted the model, you get estimates for each parameter you specified. If you opted for flat priors, these estimates will be the same but for very minor differences due to sampling in Bayesian statistics. If you did specify regularizing priors then estimates will be different, although the magnitude of that difference will depend the amount of data: the more data you have, the smaller the influence of the priors, we closer the estimates will be (see also the side note above). Importantly, both types of inferences will produce (very similar) estimates and you interpret them the same way.</p>
</div>
<div id="uncertainty-about-estimates-different-but-comparable" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Uncertainty about estimates (different but comparable)</h2>
<p>This is the point where the two approach fundamentally diverge. In case of frenquetists statistics you obtain confidence intervals and p-values based on appropriate statistics and degrees of freedom, whereas in case of Bayesian inference you obtain credible/compatibility intervals and can use posterior distribution for individual parameters to compute probability that they are strictly positive, negative, concentrated within a certain region around zero, etc.</p>
<p>These measures are conceptually different but tend to be interpreted similarly and mostly from Bayesian perspective. I think it is a good idea to compute and report all of them. Typically, they would be close, making you more certain about the results. More importantly, whenever they diverge it serves as a warning to investigate the case and what can cause this difference.</p>
</div>
<div id="model-comparison-via-information-criteria-both" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Model comparison via information criteria (both)</h2>
<p>Both approaches use information criteria to compare models with Akaike and Bayesian/Schwarz information criteria being developed specifically for the case of flat priors of frenquetist models. Here, Bayesian approach holds an advantage as the full posterior allows for more elaborate information criteria such as DIC, WAIC, or LOO. Still the core idea and the interpretation of the comparison results are the same.</p>
</div>
<div id="generating-predictions-both" class="section level2" number="7.7">
<h2><span class="header-section-number">7.7</span> Generating predictions (both)</h2>
<p>You generate predictions using the model definition which is the same for both approach. Hence, you are going to get same predictions, at least for the mean. As the two approach differ in how they characterize uncertainty, the uncertainty of predictions will be different but, typically, comparable.</p>
</div>
<div id="conclusions" class="section level2" number="7.8">
<h2><span class="header-section-number">7.8</span> Conclusions</h2>
<p>As you can see, from <em>practical</em> point of view, apart from optional Bayesian priors and different ways to quantify the uncertainty of estimates, the two approaches are the same. They require you making same decisions and the results are interpreted the same way. This lack of difference becomes even more apparent when you use software packages for running your statistical models. E.g., the way you specify your model in <code>lme4</code> (frenquetist) and <code>brms</code> (Bayesian) is very much the same to the point that in most cases you only need to change the function name (<code>lmer</code> to <code>brm</code> or vice versa) and leave the rest the same.</p>
<p>Thus, the point is that you should not be choosing between studying or doing frenquetists or Bayesian statistic. I feel more comfortable with Bayesian, mostly because it makes it easier interpret statistical significance. However, my typical approach is to start with frenquetist statistics (fast, good for shooting from a hip), once I am certain about my decisions (likelihood, model) I re-impliement the same model in Bayesian using informative priors and see whether results match (with reason). Then, I report both sets of inferences. This costs me remarkably little time precisely because there is so little difference between the two approaches from practical point of view!</p>
</div>
<div id="take-home-message-1" class="section level2" number="7.9">
<h2><span class="header-section-number">7.9</span> Take home message</h2>
<p>We are not studying something completely different! We are merely approaching it from an unusual angle that leads to deeper understanding and interesting insights in the long run.</p>

</div>
</div>


























<div class="footnotes">
<hr />
<ol start="17">
<li id="fn17"><p>Not really.<a href="bayesian-versus-frequentist-statistics.html#fnref17" class="footnote-back">â©ï¸</a></p></li>
<li id="fn18"><p>Admittedly, in this case people often start with the statistical test and see whether data is suitable rather than the other way around.<a href="bayesian-versus-frequentist-statistics.html#fnref18" class="footnote-back">â©ï¸</a></p></li>
<li id="fn19"><p>However, you do see cases when one simply throws all factors and interactions into the pot with little regard for an underlying causal model or interpretability of the coefficients.<a href="bayesian-versus-frequentist-statistics.html#fnref19" class="footnote-back">â©ï¸</a></p></li>
<li id="fn20"><p>Modern packages like <em>brms</em> make it easy for you by deducing a set of reasonable priors for you. However, it is always a good idea to double-check them.<a href="bayesian-versus-frequentist-statistics.html#fnref20" class="footnote-back">â©ï¸</a></p></li>
<li id="fn21"><p>In my experience, people tend to worry about priors for data unseen. Some kind of data of which you know absolutely nothing, hence, have trouble deducing priors. In practice, you always know something about the topic and the data. If not, you should read on it instead of using flat priors!<a href="bayesian-versus-frequentist-statistics.html#fnref21" class="footnote-back">â©ï¸</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="information-criteria.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes-on-statistical-rethinking.pdf", "notes-on-statistical-rethinking.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
