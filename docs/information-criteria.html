<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Information Criteria | Notes on Statistics</title>
<meta name="author" content="Alexander Pastukhov">
<meta name="description" content="These are notes on information criteria, presented in chapter 7 of the Statistical Rethinking book. The purpose of this note is to provide some intuition about the information criteria presented...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="Chapter 6 Information Criteria | Notes on Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="These are notes on information criteria, presented in chapter 7 of the Statistical Rethinking book. The purpose of this note is to provide some intuition about the information criteria presented...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Information Criteria | Notes on Statistics">
<meta name="twitter:description" content="These are notes on information criteria, presented in chapter 7 of the Statistical Rethinking book. The purpose of this note is to provide some intuition about the information criteria presented...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css%20-%20style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Notes on Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Precis</a></li>
<li><a class="" href="loss-functions.html"><span class="header-section-number">2</span> Loss functions</a></li>
<li><a class="" href="directed-acyclic-graphs-and-causal-reasoning.html"><span class="header-section-number">3</span> Directed Acyclic Graphs and Causal Reasoning</a></li>
<li><a class="" href="collider-bias.html"><span class="header-section-number">4</span> Collider bias</a></li>
<li><a class="" href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></li>
<li><a class="active" href="information-criteria.html"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="" href="bayesian-vs.-fequentist-statisics.html"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></li>
<li><a class="" href="mixtures.html"><span class="header-section-number">8</span> Mixtures</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">9</span> Instrumental Variables</a></li>
<li><a class="" href="parameters-combining-information-from-an-individual-with-population.html"><span class="header-section-number">10</span> Parameters: combining information from an individual with population</a></li>
<li><a class="" href="incorporating-measurement-error-a-rubber-band-metaphor.html"><span class="header-section-number">11</span> Incorporating measurement error: a rubber band metaphor</a></li>
<li><a class="" href="generalized-additive-models-as-continuous-random-effects.html"><span class="header-section-number">12</span> Generalized Additive Models as continuous random effects</a></li>
<li><a class="" href="flat-priors-the-strings-attached.html"><span class="header-section-number">13</span> Flat priors: the strings attached</a></li>
<li><a class="" href="unbiased-mean-versus-biased-variance-in-plain-english.html"><span class="header-section-number">14</span> Unbiased mean versus biased variance in plain English</a></li>
<li><a class="" href="probability-mass-versus-probability-density.html"><span class="header-section-number">15</span> Probability mass versus probability density</a></li>
<li><a class="" href="effective-degrees-of-freedom-number-of-parameters.html"><span class="header-section-number">16</span> Effective degrees of freedom / number of parameters</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alexander-pastukhov/notes-on-statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="information-criteria" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Information Criteria<a class="anchor" aria-label="anchor" href="#information-criteria"><i class="fas fa-link"></i></a>
</h1>
<p>These are notes on information criteria, presented in chapter 7 of the Statistical Rethinking book. The purpose of this note is to provide some intuition about the information criteria presented in the “Statistical Rethinking” book. I deliberately oversimplified and overgeneralized certain aspects to paint a “bigger picture.”</p>
<div id="deviance" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Deviance<a class="anchor" aria-label="anchor" href="#deviance"><i class="fas fa-link"></i></a>
</h2>
<p>In the chapter, deviance is introduced as a an estimate for KL-divergence, which in turn is a relative entropy, i.e., the difference between cross-entropy and actual entropy of events. Keep that in mind but you could look at deviance itself as a straightforward goodness-of-fit measure, similar to squared residuals and coefficient of determination <span class="math inline">\(R^2\)</span>. In both cases, you have difference between model prediction (the regression line) and an actual data point. In ordinary least squares (OLS) approach, you quantify this imperfection of prediction by squaring residuals. You sum up all residuals to get the sum of squared residuals (<span class="math inline">\(SS_{res}\)</span>) and then you can compute <span class="math inline">\(R^2\)</span> by comparing it to the total sum of residuals in the data (<span class="math inline">\(SS_{total}\)</span>):
<span class="math display">\[R^2 = 1 - \frac{SS_{res}}{SS_{total}}\]</span></p>
<p>If you use likelihood, you compute the probability that a data point comes from a distribution defined by a model. Then, you compute the (total) joint probability by multiplying all probabilities or, better still, by computing the sum of their log-transform (log likelihood). Then, you can compare the observed log likelihood to the highest theoretically possible log likelihood for a <em>saturated model</em> (<span class="math inline">\(\Theta_s\)</span>), which has as many parameters as there are data points, so that it predicts each point perfectly. This is the original definition of (total) <em>deviance</em>:
<span class="math display">\[D = -2 \cdot (log(p(y|\Theta)) - log(p(y|\Theta_s)))\]</span></p>
<p>Recall that <span class="math inline">\(log(\frac{a}{b}) = log(a) - log(b)\)</span>, so we can rearrange it and see that it is a log-ratio of likelihoods:
<span class="math display">\[D = -2 \cdot  log \left(\frac{p(y|\Theta)}{p(y|\Theta_s)}\right)\]</span>
As <span class="math inline">\(p(y|\Theta)\)</span> increases, the fraction inside get closer to 1. The <span class="math inline">\(log()\)</span> bit flips and non-linearly scales it. The minus sign flips it back and we end up with smaller numbers meaning better fit.
<img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-24-1.png" width="672"></p>
<p>The <span class="math inline">\(2\)</span> is there to facilitate significance testing for <em>nested</em> models. Models <span class="math inline">\(\Theta1\)</span> and <span class="math inline">\(\Theta2\)</span> are nested, if <span class="math inline">\(\Theta2\)</span> has all predictors of <span class="math inline">\(\Theta1\)</span> plus <em>k</em> predictors. E.g., model <span class="math inline">\(Divorce = Marriage~Rate\)</span> is nested inside <span class="math inline">\(Divorce = Marriage~Rate + Marriage~Age\)</span> with later model having 1 more parameter. Your actual model <span class="math inline">\(\Theta\)</span> is nested inside the saturated model <span class="math inline">\(\Theta_s\)</span> that has <span class="math inline">\(k = n - k_{model}\)</span> more parameters, where <span class="math inline">\(n\)</span> is sample size and <span class="math inline">\(k_{model}\)</span> is number of parameters in your model. It turns out that in this case, you can determine whether the difference in goodness-of-fit between two models is significant using chi-squared distribution with <em>k</em> degrees of freedom. The only catch is that log-ratio is half the magnitude, so you need that <span class="math inline">\(2\)</span> to match things up<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;I did not follow the derivation of that correspondence yet, so I cannot comment on how and why.&lt;/p&gt;"><sup>15</sup></a>.</p>
<p>At this point, you might remember that the definition of deviance in the book was for a single model:
<span class="math display">\[D = -2 \cdot log(p(y|\Theta))\]</span></p>
<p>Unfortunately, same term can be used both ways, to refer to a log likelihood of a single model or, technically more correctly, to the log-ratio you saw above. In reality, you will mostly see deviance defined as in the book because it is used to compare nested models via chi-square distribution as I’ve described above, with only difference that you compare any two nested models, not just to a saturated one. This is how nested models are frequently compared, for example, see <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/anova.html">anova()</a> function in R.</p>
<p>Note that a deviance for a single model still expresses the same idea of goodness-of-fit but is merely not normalized. Thus deviance as in the book <span class="math inline">\(D = -2 \cdot log(p(y|\Theta))\)</span> corresponds to sum of residuals (absolute values mean nothing but you can use them to compare two models on the same data), whereas <em>total</em> deviance corresponds to the <span class="math inline">\(R^2\)</span> (values are directly interpretable, theoretically, you can use them to compare models fit on different samples).</p>
</div>
<div id="general-idea-information-criteria-as-miles-per-gallon" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> General idea: information criteria as miles-per-gallon<a class="anchor" aria-label="anchor" href="#general-idea-information-criteria-as-miles-per-gallon"><i class="fas fa-link"></i></a>
</h2>
<p>The general formula for all information criteria discussed below is
<span class="math display">\[-2\cdot log \left( \frac{goodness~of~fit}{model~complexity} \right)\]</span></p>
<p>The goodness-of-fit in the numerator is the likelihood, the joint probability of observing the model given each data point <span class="math inline">\(\prod_i p(\Theta|y_i)\)</span>. The denominator expresses model complexity, i.e., its flexibility in fitting the sample and, therefore, its tendency to overfit. Thus, the fraction itself is <em>goodness-of-fit per unit of model complexity</em>. This is like miles-per-gallon for car efficiency, so better models are more efficient models, churning out more goodness per complexity.</p>
<p>The numerator is the same for all information criteria discussed below and they differ only in how they compute the model complexity.</p>
</div>
<div id="akaike-information-criterion-aic" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Akaike Information Criterion (AIC)<a class="anchor" aria-label="anchor" href="#akaike-information-criterion-aic"><i class="fas fa-link"></i></a>
</h2>
<p>The formula most commonly used is
<span class="math display">\[ AIC = -2\cdot log(p(\Theta|y)) + 2k \]</span>
where <span class="math inline">\(k\)</span> is the number of parameters of the model. If you are not a mathematician who is used to translate back-and-forth using logarithms, you may fail spot to the ratio I was talking about earlier. For this, you need to keep in mind that <span class="math inline">\(log(\frac{a}{b}) = log(a) - log(b)\)</span> and that <span class="math inline">\(a = log(exp(a))\)</span>. Let us re-arrange a bit to get the log-ratio back:
<span class="math display">\[ AIC = -2\cdot log(p(\Theta|y)) + 2k \]</span>
<span class="math display">\[ AIC = -2 (log(p(\Theta|y)) - k)\]</span>
<span class="math display">\[ AIC = -2 (log(p(\Theta|y)) - log(exp(k))\]</span>
<span class="math display">\[ AIC = -2 \cdot log \left(\frac{p(\Theta|y)}{exp(k)} \right)\]</span>
And here it is, the log-ratio I’ve promised! As you can see, AIC assumes that model complexity grows exponentially with the number of parameters. In other words, adding another parameter increases complexity ~<span class="math inline">\(2.718\)</span> times.</p>
<p>If you are to use AIC, the current recommendation is to <em>correct it</em> with an extra penalty for the size of the sample
<span class="math display">\[AICc = AIC + \frac{2k^2 + 2k}{n - k - 1}\]</span>
where <span class="math inline">\(n\)</span> is the sample size. I won’t do it here but you should be able to work out how it is added into the exponent in the denominator.</p>
</div>
<div id="bayesian-information-criterion-bic" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Bayesian information criterion (BIC)<a class="anchor" aria-label="anchor" href="#bayesian-information-criterion-bic"><i class="fas fa-link"></i></a>
</h2>
<p>A.k.a. Schwarz information criterion (SIC, SBC, SBIC). The motivation is the same as with AIC but the complexity term, in addition to the number of parameters, also reflects the sample size <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[BIC = -2\cdot log(p(\Theta|y)) + log(n) k \]</span></p>
<p>Let us do re-arranging again
<span class="math display">\[BIC = -2\cdot log(p(\Theta|y)) + log(n) k \]</span>
<span class="math display">\[BIC = -2 \left( log(p(\Theta|y)) + log(n) \frac{k}{2} \right) \]</span>
<span class="math display">\[BIC = -2 \left( log(p(\Theta|y)) - log \left(exp(log(n) \cdot \frac{k}{2} \right) \right) \]</span></p>
<p>For the complexity term, we need to keep in mind that <span class="math inline">\(exp(a \cdot b) = exp(a)^b\)</span>. Thus,
<span class="math display">\[exp \left(log(n) \cdot \frac{k}{2} \right)= exp(log(n)) ^ \frac{k}{2} = n^\frac{k}{2}\]</span>
Putting the complexity term back, we get
<span class="math display">\[BIC = -2 \left( log(p(\Theta|y)) - log \left(n^\frac{k}{2} \right) \right)\]</span>
<span class="math display">\[BIC = -2 \cdot log \left(\frac{p(\Theta|y)}{n^\frac{k}{2}} \right)\]</span>
Thus, we end up with very similar power law complexity term which uses sample size instead of Euler’s number as the base.</p>
</div>
<div id="problem-of-aic-and-bic-one-size-may-not-fit-all" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Problem of AIC and BIC: one size may not fit all<a class="anchor" aria-label="anchor" href="#problem-of-aic-and-bic-one-size-may-not-fit-all"><i class="fas fa-link"></i></a>
</h2>
<p>Both AIC and BIC assume that model complexity and flexibility, that leads to overfitting, is reflected in the number of parameters <em>k</em>. However, this is a fairly indirect measure of a model flexibility, based on how models in general tend to overfit data in general. But you probably want to know how your specific model uses its parameters to fit your specific sample and how much overfitting you should expect in that specific case. Because even if a parameter is present in the model, it may not be able to fully use it in case of regularization or multilevel models.</p>
<p>Regularization, in form of strong priors, lasso/ridge regression, etc., restricts the range of values that a given parameter can take. Thus, a model cannot exploit it as much as <em>other</em> parameters and will be less able to use it to improve fit to the sample. Similarly, in hierarchical multilevel modeling, you may have dozens or hundreds of parameters that describe intercepts and/or slopes for individual participants (random factors, in general), but most of them could be trivially average (same as or very similar to the group average) and contribute little to the actual fit. In these cases, a simple raw count, which treats all parameters as equals, will overestimate model complexity.</p>
<p>The desire to go beyond one-size-fits-all approach and be as model- and data-specific led to development of deviance information criterion (DIC) and widely-applicable information criterion (WAIC). Both use posterior distribution of <em>in-sample</em> deviance and base their penalty on how <em>variable</em> this posterior distribution is. Higher variance, meaning that a model produces very different fits ranging from excellent to terrible, hints that model is too flexible for its own good and leads to higher complexity estimate (penalty for overfitting). Conversely, very similar posterior deviance (low variance) means that model is too restricted to fine-tune itself to the sample and its complexity is low.</p>
<p>If you understand why variance of the posterior distribution of divergence is related to models’ flexibility and, therefore, to the number of effective parameters, just skip the next section. If not, I came up with a musical instruments metaphor that I and at least some people I’ve tested it upon found useful.</p>
</div>
<div id="musical-instruments-metaphor" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Musical instruments metaphor<a class="anchor" aria-label="anchor" href="#musical-instruments-metaphor"><i class="fas fa-link"></i></a>
</h2>
<p>Imagine that you are trying to play a song that you have just heard. But the only instrument you have is a triangle. This is not a particularly flexible instruments pitch-wise, so your rendition of that song will not be very good (your model underfits the data). The good news is that even if you do your worst and do not really try, no one will notice because your worst performance will sound very much like your best one. Simply because it is very hard to make songs sound different using a triangle. Thus, if you play that song many times, trying different versions of it with us judging how close you are to the original, the score we will give you will be very similar (low variance of the deviance for fitting to sample).</p>
<p>But what if I give you an instrument that can vary the pitch at least a bit, like a xylophone for children. Now you more expressive freedom and your version of the music will sound much more like the original. But, it also gives you an opportunity to make a mess of it, so your rendition might sound nothing like the music you’ve just heard. Thus, the instrument increases the difference between the best and the worst possible performance, so the variance of your performances (on how close they are to the original) also increases (higher variance of the deviance). A more flexible instrument will make the difference even bigger. Think violin or trombone which are not restricted to the scale, so you can play any sound in between and you can match the music you just heard exactly. Imagine that the music your just hear has an odd off-the-scale sounds. Was a defect of the turntable, which cannot go at constant speed, so overall pitch wobbles overtime (noise)? Or is it an experimental music piece designed to sound odd (signal)? If you do not know for sure, you will try to play as close to the original music your heard as possible, matching those off-scale sounds. And, because you can play any sound, your range of possible performance is even larger from one-to-one to “please, have mercy and stop!” (even larger variance of deviance).</p>
<p>In short, variance of your performance (posterior divergence) reflects how flexible your instrument is. But why is it indicative of the <em>effective</em> number of parameters? Here are regularizing priors in the world of music instrument metaphor. Imagine that in addition to the triangle, I also give you a rubber bell (it was used in a super-secret meeting in Stanislaw Lem’s Eleventh Voyage of Ijon Tichy, so that no one would hear when they ring that bell!). Now, technically you have two instruments (your number of parameters is 2) but that bell does not affect your performance (we put very strong regularizing priors so that coefficients are zero or something very-very close to zero). Thus, your ability to play the song did not change and your variance of performance stays the same. Two actual instruments, but only one “effective” one. Or, I give you a piano but allow you to use only one octave and only white keys. Yes, you have a piano but with this regularization it is as complex as as kids’ xylophone. The <em>potential</em> number of notes you can play is great (AIC and BIC would be very impressed and slap a heavy penalty on you) but the actual “effective” range is small. Or, you regularize yourself to play scale only notes using violin (something you learn to do). In all of these cases, you deliberately restrict yourself. But why? Why not just play as you heard it (why not fit as well as you can?)? Because if the song you heard is short (sample is small), regularization based on your knowledge about real life helps you to ignore the noise that is always present. E.g., you know that song is for kids’ xylophone, so even if you heard notes outside of a single octave that was probably a problem with recording. Or, you never heard that piece for violin but you do know other works of this composer and they always use scale-only notes, so you should not use my violin to play off-scale sounds.</p>
<p>Multilevel models also limit the actual use of parameters. Imagine you heard a recording of a symphonic orchestra. Lots of violins but you figured out that most of them actually play the same melody. So you can get away with using one violin score (sample group average) and assume that most violins play like that (most participants are very close to group average). Any deviations from that group melody are probably mistakes by individual musicians, not the actual melody. Same goes if you hear a choir. Again, many people sing (lots of parameters!) but, mostly, in unison, so you do not need to create an individual score sheet (parameter) for each singer, just one per group of singers.</p>
<p>Wrapping up the metaphor, the more flexible your instrument is, the more variable your performance can be, the easier it is for you to mimic noise and imperfections of the recording that have nothing to do with the piece itself. But when you play it next time, matching the recording with all its noise and distortions perfectly, people who know the piece will be puzzled or may not even recognize it (poor out-of-sample predictions). Adopting the melody for a more limited instrument may make it easier for others to recognize the song! Thus, higher variance in performance accuracy (higher variance of deviance) indicates that you can overfit easily with that instrument (model) and you should be extra careful (impose higher penalty for complexity).</p>
</div>
<div id="deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic" class="section level2" number="6.7">
<h2>
<span class="header-section-number">6.7</span> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)<a class="anchor" aria-label="anchor" href="#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><i class="fas fa-link"></i></a>
</h2>
<p>The two are very similar, as both compute the model complexity based on posterior distribution of log likelihood. The key difference is that DIC sums the log likelihood for each model (sample) first and then computes the variance. WAIC computes variance of log likelihood per point and then sums those variances up. In the musical instrument metaphor, for DIC you perform the piece many times (generate many posterior samples), compute accuracy for each performance (deviance for a single sample), and then compute how variable they are. For WAIC, you go note by note (observation by observation). For each note you compute variance over all sample to see how consistent you are in playing it. Then, you sum this up.</p>
<p><span class="math display">\[DIC = -2 \cdot \left( log(p(y|\Theta)) - var(\sum log(p(y|\Theta_i))) \right)\]</span>
<span class="math display">\[WAIC = -2 \cdot \left( log(p(y|\Theta)) - \sum_i var(log(p(y_i|\Theta))) \right)\]</span></p>
<p>The penalty replace <span class="math inline">\(k\)</span> in AIC and, therefore, will go into the exponent inside the ratio. Again, same idea, that increase in variance of deviance (either per sample in DIC or per point in WAIC) leads to exponentially increasing estimate of complexity.</p>
<p>WAIC is more stable mathematically and is mode widely applicable (that’s what statisticians tell us). Moreover, its advantage is that it explicitly recognizes that not all data points in your sample are equal. Some (outliers) are much harder to predict than others. And it is variance of log likelihood for these points that determines how much your model can overfit. An inflexible model will make a poor but consistent job (triangles don’t care about pitch!), whereas a complex model can do anything from spot-on to terrible (violins can do anything). In short, you should use WAIC yourself but recognize DIC when you see it and think of it as somewhat less reliable WAIC, which is still better than AIC or BIC when you use regularizing priors and/or hierarchical models.</p>
</div>
<div id="importance-sampling" class="section level2" number="6.8">
<h2>
<span class="header-section-number">6.8</span> Importance sampling<a class="anchor" aria-label="anchor" href="#importance-sampling"><i class="fas fa-link"></i></a>
</h2>
<p>Importance sampling is mentioned in the chapter but is never explained, so here is a brief description. The core idea is to pretend that you sample from a distribution you need (but have no access to) by sampling from another distribution (the one you have access to) and “translating” the probabilities via <em>importance ratios</em>. What does this mean?</p>
<p>Imagine that you want to know an average total score for a given die after you throw it ten times. The procedure is as simple as it gets: you toss the die ten times, record the number you get on each throw, sum them up at the end. Repeat the same toss-ten-times-and-sum-it-up as many times as you want and compute your average. But what if you do not have access to that die because it is <em>the die</em> and is kept under lock in International Bureau of Weights and Measures? Well, you have <em>a die</em> which you can toss and you have a list of <em>importance ratios</em> for each number. These <em>important ratios</em> tell you how much more likely is the number of <em>the die</em> (the one you are after) compared to <em>a die</em> you have in your hand. Let’s say the importance ratio for <em>1</em> (so, number 1 comes up on top) is <code>3.0</code>. This means that whenever <em>your die</em> gives you <em>1</em>, you assume that <em>the die</em> came up <em>1</em> on <strong>three</strong> throws. If the importance ratio for <em>2</em> is 0.5, whenever you see <em>2</em> on your die, you record only half the throw (<em>2</em> comes up twice as rarely for real die than for your die, so two throws that give you <em>2</em> amount to a single throw). This way you can toss your die and every toss equates to different number of throws that generated the same number for <em>the die</em>. So, you sample your die but record outcomes for the other die. Funny thing is that you don’t even need to know how fair your die is and what is the probability of individual sides. As long as you know the importance ratios, keep tossing it and translating the probabilities, you will get the samples for the die you are interested in.</p>
<p>Note that if you toss your die ten times, the translated number of tosses for <em>the die</em> does not need to add up to ten. Imagine that, just by chance, you got <em>1</em> four times. Given the importance ratio of <code>3.0</code> that alone translates into twelve tosses. Solution? You <em>normalize</em> your result by <em>sum of importance ratios</em> and get back you ten tosses.</p>
<p>The very obvious catch is, <em>how do we know the importance ratios</em>? Well, that is situation specific. Sometimes, we can compute them because we know both distributions, it just that one is easier to sample than the target one (so, we optimize the use of computing power). Sometimes, as in case of PSIS/LOO below, we can use an approximation.</p>
</div>
<div id="pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo" class="section level2" number="6.9">
<h2>
<span class="header-section-number">6.9</span> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)<a class="anchor" aria-label="anchor" href="#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><i class="fas fa-link"></i></a>
</h2>
<p>The importance sampling I’ve described above is the key to the PSIS/LOO. The idea is the same, we want to sample from the posterior of the model that was fitted <em>without</em> a specific data point <span class="math inline">\(y_i\)</span> (we write it as <span class="math inline">\(p(\Theta_{-i,s}|y_i)\)</span>). But we do not really want to refit the model. Instead, we want to use what we already have, samples from the model that was fit on all the data, <em>including</em> point <span class="math inline">\(y_i\)</span>. So, we pull of the same importance sampling trick, sampling from <span class="math inline">\(p(\Theta_s|y_i)\)</span> and translating it to <span class="math inline">\(p(\Theta_{-i,s}|y_i)\)</span>. The only thing we need are importance factors<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Unfortunately, I did not yet follow the derivation of this proportionality, so I cannot explain why this is the case mathematically.&lt;/p&gt;"><sup>16</sup></a>
<span class="math display">\[r_i = \frac{p(\Theta_{-i,s}|y_i)}{p(\Theta_s|y_i)} \propto \frac{1}{p(y_i|\Theta_s)}\]</span></p>
<p>The importance ratio tells you that decrease in your ability to predict an observation out-of-sample is proportional to difficulty of predicting in-sample. Here is an intuition behind this. <em>Any</em> observation will be harder to predict, if it was not included into the data the model was trained on. This is because, in its absence the model will use its parameters to fit the data that is present, including fitting noise, if it has spare parameters. So, you expect that out-of-sample deviance (<span class="math inline">\(p(y_i|\Theta_{-i}\)</span>) should be always worse than in-sample deviance for the same observation (<span class="math inline">\(p(y_i|\Theta\)</span>). How much worse depends on how “typical” the observation is. If it is typical and “easy” for a model to predict, in its absence the model will still see many similar “typical” observations and will be well prepared to predict it. However, if the observations is atypical, an outlier, the model won’t see too many observations that are alike and will concentrate more on typical points.</p>
<p>I.e., assume that out-of-sample deviance is in-sample deviance squared (the importance ratio we use goes in that direction). Asume that our <em>in-sample</em> deviance is 60, 40, and 50 in one case and 10, 10 and 130 in the other case. Both deviances sum up to 150, so from <em>in-sample</em> point of view they are equal. However, approximating <em>out-of-sample</em> deviance as a square of <em>in-sample</em> deviance makes for a dramatic difference. For the first case, it jumps to 7700 but the second case to 17100, more than twice the difference. This is because in second case <em>out-of-sample</em> deviance is dominanted by that single very difficult point, which will be terribly difficult out-of-sample.</p>
<p>This explosive powerlaw-like scaling of expected out-of-sample deviance is the reason for warnings that LOO/PSIS generates. A single very difficult observation can be single-handedly responsible for most of the deviance, making it hard to understand whether the model is good or bad on the rest of observations. Thus, you should consider what this extreme outlier means for your understanding of the process. Is it so different because it was recorded wrongly (the responsible person pressed 1 instead of 7)? Is this an extreme response time because a participant was distracted on that trial and started paying attention to the task only 15 seconds after the trial began? Is it an important observations that indicates that you are ignoring a really important predictor? You should think carefully about it and decide on whether you treat that observation differently (e.g., allowing for larger uncertainty for that point because you think this is mostly noise) or you change your model to better account for it.</p>
</div>
<div id="bayes-factor" class="section level2" number="6.10">
<h2>
<span class="header-section-number">6.10</span> Bayes Factor<a class="anchor" aria-label="anchor" href="#bayes-factor"><i class="fas fa-link"></i></a>
</h2>
<p>Not an information criterion. However, it is a popular way to compare Bayesian models. Compared to information criteria, the logic is reversed. In case of the information criteria, we are asking which <strong>model fits data</strong> the best given the penalty we impose for its complexity. In case of Bayes Factor, we already have two models (could be different models with different number of parameters or just with different parameter values) and we are interested how well the <strong>data matches models</strong> we already have.</p>
<p>Let’s start with the Bayes theorem:
<span class="math display">\[Pr(M|D)=\frac {\Pr(D|M)\Pr(M)}{\Pr(D)}\]</span>
where, <em>D</em> is data and <em>M</em> is the model (hypothesis). The tricky part is the marginal probability (prior) of data <span class="math inline">\(Pr(D)\)</span>. We hardly ever know it for sure, making computing the “correct” value for <span class="math inline">\(Pr(M|D)\)</span> problematic. When using posterior sampling, we side-step the issue by ignoring it and normalizing the posterior by the sum of the posterior distribution. Alternatively, when comparing two models, you can compute their ratio:
<span class="math display">\[{\frac {\Pr(D|M_{1})}{\Pr(D|M_{2})}}={\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}} \cdot {\frac {\Pr(M_{1})}{\Pr(M_{2})}}\]</span>
here <span class="math inline">\(\frac{\Pr(D|M_{1})}{\Pr(D|M_{2})}\)</span> are <strong>posterior odds</strong>, <span class="math inline">\(\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}\)</span> is <strong>Bayes Factor</strong>, and <span class="math inline">\(\frac {\Pr(M_{2})}{\Pr(M_{1})}\)</span> are <strong>prior odds</strong>. The common <span class="math inline">\(Pr(D)\)</span> nicely cancels out!</p>
<p>If you assume that both hypotheses/models are equally likely (you have flat priors), the prior odds are 1:1 and your posterior odds are equal to Bayes Factor or, vice versa, Bayes Factor is equal to posterior odds. This means you can just pick their likelihoods from the posterior sampled distribution and compute the ratio.</p>
<p>I am not a big fan of Bayes Factor for conceptual reasons. Although it can compare any two models (as long as the sample is the same), it looks a lot like a Bayesian version of a p-value and, therefore, lends itself naturally to the null-hypothesis testing. And, as far as my reading of literature in my field is concerned, this is how people most frequently use it, as a cooler Bayesian way of null-hypothesis testing. You have no worries about multiple comparisons (it is Bayesian, so no need for error correction!) and it can prove null hypothesis (it is the ratio, so flip it and see how much stronger <em>H0</em> is)! There is nothing wrong with this per se but the advantage of Bayesian statistics and information criteria is that you do not <em>need</em> to think in terms of null hypothesis testing and nested models. Adopting Bayes Factor may prevent you from seeing this and will allow you to continue doing same analysis just in a differently colored wrapper. Again, there is nothing wrong with exploratory analysis using null hypothesis testing until you can formulate a better model. But it should not be the <em>only</em> way you approach modeling.</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></div>
<div class="next"><a href="bayesian-vs.-fequentist-statisics.html"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#information-criteria"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="nav-link" href="#deviance"><span class="header-section-number">6.1</span> Deviance</a></li>
<li><a class="nav-link" href="#general-idea-information-criteria-as-miles-per-gallon"><span class="header-section-number">6.2</span> General idea: information criteria as miles-per-gallon</a></li>
<li><a class="nav-link" href="#akaike-information-criterion-aic"><span class="header-section-number">6.3</span> Akaike Information Criterion (AIC)</a></li>
<li><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="header-section-number">6.4</span> Bayesian information criterion (BIC)</a></li>
<li><a class="nav-link" href="#problem-of-aic-and-bic-one-size-may-not-fit-all"><span class="header-section-number">6.5</span> Problem of AIC and BIC: one size may not fit all</a></li>
<li><a class="nav-link" href="#musical-instruments-metaphor"><span class="header-section-number">6.6</span> Musical instruments metaphor</a></li>
<li><a class="nav-link" href="#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><span class="header-section-number">6.7</span> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)</a></li>
<li><a class="nav-link" href="#importance-sampling"><span class="header-section-number">6.8</span> Importance sampling</a></li>
<li><a class="nav-link" href="#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><span class="header-section-number">6.9</span> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)</a></li>
<li><a class="nav-link" href="#bayes-factor"><span class="header-section-number">6.10</span> Bayes Factor</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alexander-pastukhov/notes-on-statistics/blob/master/05-information-criteria.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alexander-pastukhov/notes-on-statistics/edit/master/05-information-criteria.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Notes on Statistics</strong>" was written by Alexander Pastukhov. It was last built on 2021-11-10.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
